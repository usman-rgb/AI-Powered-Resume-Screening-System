{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_fdRJUVlrHrO"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber pytesseract Pillow tensorflow nltk pandas numpy scikit-learn\n",
        "!sudo apt install -y tesseract-ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kqIUiucKGC1Z"
      },
      "outputs": [],
      "source": [
        "!apt install tesseract-ocr -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zd_7CtkUGMp5",
        "outputId": "40e2bded-09a1-42bd-947f-41e89f7da7ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uR7vH39dGWWZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DU--i43ZKWHl"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/My\\ Drive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wCouYthfEDJ5",
        "outputId": "41bec673-7584-4aff-840c-ba6630cb0b26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "Dataset loaded successfully!\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "634/634 - 921s - 1s/step - accuracy: 0.3909 - loss: 2.3909 - val_accuracy: 0.9756 - val_loss: 0.2181\n",
            "Epoch 2/10\n",
            "634/634 - 918s - 1s/step - accuracy: 0.8981 - loss: 0.3583 - val_accuracy: 1.0000 - val_loss: 0.0097\n",
            "Epoch 3/10\n",
            "634/634 - 917s - 1s/step - accuracy: 0.9713 - loss: 0.1165 - val_accuracy: 1.0000 - val_loss: 0.0016\n",
            "Epoch 4/10\n",
            "634/634 - 914s - 1s/step - accuracy: 0.9850 - loss: 0.0642 - val_accuracy: 1.0000 - val_loss: 8.3236e-04\n",
            "Epoch 5/10\n",
            "634/634 - 912s - 1s/step - accuracy: 0.9895 - loss: 0.0449 - val_accuracy: 1.0000 - val_loss: 5.2622e-04\n",
            "Epoch 6/10\n",
            "634/634 - 911s - 1s/step - accuracy: 0.9917 - loss: 0.0312 - val_accuracy: 1.0000 - val_loss: 1.2154e-04\n",
            "Epoch 7/10\n",
            "634/634 - 921s - 1s/step - accuracy: 0.9934 - loss: 0.0267 - val_accuracy: 1.0000 - val_loss: 9.8550e-05\n",
            "Epoch 8/10\n",
            "634/634 - 933s - 1s/step - accuracy: 0.9946 - loss: 0.0214 - val_accuracy: 1.0000 - val_loss: 5.7908e-05\n",
            "Epoch 9/10\n",
            "634/634 - 913s - 1s/step - accuracy: 0.9931 - loss: 0.0256 - val_accuracy: 1.0000 - val_loss: 4.3109e-05\n",
            "Epoch 10/10\n",
            "634/634 - 915s - 1s/step - accuracy: 0.9959 - loss: 0.0182 - val_accuracy: 1.0000 - val_loss: 1.6019e-05\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, SpatialDropout1D, Dense, Dropout\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# NLTK Download\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load Dataset\n",
        "\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "data = pd.read_csv('/content/drive/My Drive/job_descriptions Short Form.csv')\n",
        "print(\"Dataset loaded successfully!\")\n",
        "\n",
        "# Function to extract text from PDF\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_file) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "# Function to extract text from images\n",
        "\n",
        "\n",
        "def extract_text_from_image(image_file):\n",
        "    image = Image.open(image_file)\n",
        "    text = pytesseract.image_to_string(image)\n",
        "    return text.strip()\n",
        "\n",
        "# Text Preprocessing Function\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
        "    words = text.split()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply Preprocessing\n",
        "\n",
        "\n",
        "data['job_description'] = data['Job Description'].apply(preprocess_text)\n",
        "data['skills'] = data['skills'].apply(preprocess_text)\n",
        "data['experience'] = data['Experience'].apply(preprocess_text)\n",
        "data['qualifications'] = data['Qualifications'].apply(preprocess_text)\n",
        "data['work_type'] = data['Work Type'].apply(preprocess_text)\n",
        "data['preference'] = data['Preference'].apply(preprocess_text)\n",
        "data['country'] = data['Country'].apply(preprocess_text)\n",
        "\n",
        "data['text_features'] = data['job_description'] + ' ' + data['skills'] + ' ' + data['experience'] + ' ' + data['qualifications'] + ' ' + data['work_type'] + ' ' + data['preference'] + ' ' + data['country']\n",
        "\n",
        "# Tokenization\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['text_features'])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X = tokenizer.texts_to_sequences(data['text_features'])\n",
        "X = pad_sequences(X, padding='post', maxlen=200)\n",
        "\n",
        "# Encode Job Titles\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data['Job Title'])\n",
        "\n",
        "# Compute Class Weights\n",
        "\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# Train-Test Split\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build Model\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=100, input_length=200),\n",
        "    SpatialDropout1D(0.2),\n",
        "    Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(np.unique(y)), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), class_weight=class_weight_dict, verbose=2)\n",
        "\n",
        "# Save Matching Resumes\n",
        "\n",
        "\n",
        "def save_matching_resumes(resume_text, file_name):\n",
        "    with open(file_name, 'a') as file:\n",
        "        file.write(resume_text + '\\n---\\n')\n",
        "\n",
        "# Load and process multiple resumes\n",
        "\n",
        "\n",
        "resume_files = []\n",
        "processed_resumes = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WVec3HKb7Coc"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit pyngrok pandas numpy tensorflow scikit-learn nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhoFVWAc7OXM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHmFBtkY7QHj"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Ngrok ka authentication token replace karein apne khud ke token se\n",
        "!ngrok authtoken 2syOYmEY22b8ICyj6vDwyHAveII_6fRqGLJjdtsjTK2BoYSNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KDUSWKqCw8HD"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2rO00Lw6nYF"
      },
      "outputs": [],
      "source": [
        "%%writefile UsmanResume.py\n",
        "\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Page Configuration\n",
        "\n",
        "\n",
        "st.set_page_config(page_title='AI Resume Screening', layout='wide')\n",
        "\n",
        "# Custom CSS for Styling\n",
        "\n",
        "\n",
        "st.markdown('''<style>\n",
        "body {background-color: #f5f5f5;}\n",
        ".main {padding: 2rem 5rem;}\n",
        ".header {font-size: 2.5rem; color: #333333; font-weight: 700; text-align: center; margin-bottom: 2rem;}\n",
        ".card {background: white; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); padding: 2rem; margin-bottom: 2rem;}\n",
        ".card h3 {color: #2c3e50;}\n",
        ".button {background-color: #3498db; color: white; border: none; padding: 0.5rem 1rem; border-radius: 5px; cursor: pointer;}\n",
        ".button:hover {background-color: #2980b9;}\n",
        "</style>''', unsafe_allow_html=True)\n",
        "\n",
        "# Load Dataset\n",
        "\n",
        "\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    data = pd.read_csv('/content/drive/My Drive/job_descriptions Short Form.csv')\n",
        "    return data\n",
        "\n",
        "# Sidebar Navigation\n",
        "\n",
        "\n",
        "st.sidebar.title(\"Usman-AI Resume Screening System\")\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "data = load_data()\n",
        "\n",
        "# All Jobs Section\n",
        "\n",
        "\n",
        "st.sidebar.header(\"All Job Titles\")\n",
        "all_jobs = data['Job Title'].unique()\n",
        "st.sidebar.write(all_jobs)\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "# All Skills Section\n",
        "\n",
        "\n",
        "st.sidebar.header(\"All Skills\")\n",
        "all_skills = data['skills'].str.split(',').explode().unique()\n",
        "st.sidebar.write(all_skills)\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "# All Qualifications Section\n",
        "\n",
        "\n",
        "st.sidebar.header(\"All Qualifications\")\n",
        "all_qualifications = data['Qualifications'].str.split(',').explode().unique()\n",
        "st.sidebar.write(all_qualifications)\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "# Job Criteria Input\n",
        "\n",
        "\n",
        "st.sidebar.header(\"Job Criteria\")\n",
        "job_title = st.sidebar.text_input(\"Job Title\", \"All Jobs\")\n",
        "skills_required = st.sidebar.text_area(\"Required Skills\", \"All Skills\")\n",
        "experience_required = st.sidebar.text_input(\"Minimum Experience\", \"2 Years\")\n",
        "qualifications_required = st.sidebar.text_area(\"Qualifications\", \"All Qualifications\")\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "# File Upload\n",
        "\n",
        "\n",
        "st.sidebar.header(\"Upload Resumes\")\n",
        "uploaded_files = st.sidebar.file_uploader(\"Upload resumes (PDF/Images)\", type=[\"pdf\", \"jpg\", \"png\"], accept_multiple_files=True)\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "# Main Content\n",
        "\n",
        "\n",
        "st.markdown(\"<div class='header'>Usman-AI-Powered Resume Screening System</div>\", unsafe_allow_html=True)\n",
        "st.markdown(\"Automatically screen resumes based on job criteria!\")\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# Extract Text Functions\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_file) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "def extract_text_from_image(image_file):\n",
        "    image = Image.open(image_file)\n",
        "    text = pytesseract.image_to_string(image)\n",
        "    return text.strip()\n",
        "\n",
        "# Process Resumes\n",
        "\n",
        "\n",
        "processed_resumes = []\n",
        "resume_files = []\n",
        "if uploaded_files:\n",
        "    for uploaded_file in uploaded_files:\n",
        "        file_extension = uploaded_file.name.split(\".\")[-1].lower()\n",
        "        if file_extension == \"pdf\":\n",
        "            text = extract_text_from_pdf(uploaded_file)\n",
        "        elif file_extension in [\"jpg\", \"png\"]:\n",
        "            text = extract_text_from_image(uploaded_file)\n",
        "        else:\n",
        "            continue\n",
        "        processed_resumes.append(text)\n",
        "        resume_files.append(uploaded_file.name)\n",
        "\n",
        "# Matching resumes with job criteria\n",
        "\n",
        "\n",
        "if st.sidebar.button(\"Process Resumes\") and processed_resumes:\n",
        "    job_criteria = f\"{job_title} {skills_required} {experience_required} {qualifications_required}\"\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    all_texts = [job_criteria] + processed_resumes\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "    eligible_resumes = []\n",
        "\n",
        "    for i in range(1, len(processed_resumes) + 1):\n",
        "        similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[i])[0][0] * 100\n",
        "        if similarity >= 70:\n",
        "            eligible_resumes.append(resume_files[i - 1])\n",
        "\n",
        "    # Display Results\n",
        "\n",
        "\n",
        "    st.markdown(\"<div class='card'><h3>Eligible Resumes</h3>\", unsafe_allow_html=True)\n",
        "    if eligible_resumes:\n",
        "        for resume in eligible_resumes:\n",
        "            st.markdown(f\"<p>{resume}</p>\", unsafe_allow_html=True)\n",
        "    else:\n",
        "        st.markdown(\"<p>No resumes matched the criteria.</p>\", unsafe_allow_html=True)\n",
        "    st.markdown(\"</div>\", unsafe_allow_html=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqjjXtqX7kNm"
      },
      "outputs": [],
      "source": [
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a4H5JMi7nSc"
      },
      "outputs": [],
      "source": [
        "!curl https://loca.lt/mytunnelpassword\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5By-Zlb-y1k"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model = Sequential()  # Apna actual model yahan define karo\n",
        "model.save(\"job_description_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WxwN8J3T7p_v"
      },
      "outputs": [],
      "source": [
        "!streamlit run UsmanResume.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}